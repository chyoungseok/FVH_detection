{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907cb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data_utils import load_all_slices_from_tree, SliceDataset2p5DMasked, _compute_runs\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "def split_subjectwise_indices_ordered(subject_id, test_ratio=0.3, val_ratio=0.1, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    uniq_sid, first_pos = np.unique(subject_id, return_index=True)\n",
    "    sid_in_order = uniq_sid[np.argsort(first_pos)]\n",
    "    sid_shuffled = sid_in_order.copy(); rng.shuffle(sid_shuffled)\n",
    "\n",
    "    n = len(sid_shuffled)\n",
    "    n_test = int(round(n*test_ratio))\n",
    "    n_val  = int(round((n - n_test)*val_ratio))\n",
    "\n",
    "    test_s  = set(sid_shuffled[:n_test])\n",
    "    remain  = sid_shuffled[n_test:]\n",
    "    val_s   = set(remain[:n_val])\n",
    "    train_s = set(remain[n_val:])\n",
    "\n",
    "    idx_all = np.arange(len(subject_id))\n",
    "    def expand(sids):\n",
    "        mask = np.isin(subject_id, list(sids))\n",
    "        return idx_all[mask]  # ✅ 원본 전역 인덱스 오름차순\n",
    "\n",
    "    return expand(train_s), expand(val_s), expand(test_s)\n",
    "\n",
    "def apply_indices(X, y, sep_mask, idx):\n",
    "    return X[idx], y[idx], sep_mask[idx]\n",
    "\n",
    "def make_valid_centers(X, y, sep_mask, k=1, pad_mode=\"edge\", drop_mixed=False):\n",
    "    \"\"\"subject 경계/라벨 혼합 규칙을 만족하는 유효 중심 인덱스 배열 생성.\"\"\"\n",
    "    # runs: [(start,end), ...], run_id[i]: i가 속한 run 번호\n",
    "    runs, run_id = _compute_runs(sep_mask)\n",
    "    centers = []\n",
    "    for i in range(len(X)):\n",
    "        rid = run_id[i]\n",
    "        s, e = runs[rid]\n",
    "        idxs = np.arange(i-k, i+k+1)\n",
    "\n",
    "        # 경계 처리\n",
    "        if pad_mode == \"edge\":\n",
    "            idxs = np.clip(idxs, s, e)\n",
    "            valid_mask = np.ones_like(idxs, dtype=bool)\n",
    "        else:  # zero padding\n",
    "            valid_mask = (idxs >= s) & (idxs <= e)\n",
    "\n",
    "        if drop_mixed:\n",
    "            # zero padding이면 경계 밖은 제외하고 비교\n",
    "            cand = idxs[valid_mask]\n",
    "            if not np.all(y[cand] == y[i]):\n",
    "                continue\n",
    "\n",
    "        centers.append(i)\n",
    "\n",
    "    return np.array(centers, dtype=np.int64)\n",
    "\n",
    "def undersample_on_centers(centers, y, neg_ratio=1.0, seed=42):\n",
    "    \"\"\"센터 인덱스 배열에 대해 라벨 기준 언더샘플링(재현성 보장).\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    yc = y[centers]\n",
    "    pos_c = centers[yc == 1]\n",
    "    neg_c = centers[yc == 0]\n",
    "\n",
    "    P = len(pos_c)\n",
    "    keep_neg = min(int(neg_ratio * P), len(neg_c))\n",
    "    if P == 0 or keep_neg <= 0:\n",
    "        return centers  # 그대로 반환(혹은 예외 처리)\n",
    "\n",
    "    sel_neg = rng.choice(neg_c, size=keep_neg, replace=False)\n",
    "    keep = np.concatenate([pos_c, sel_neg])\n",
    "    rng.shuffle(keep)\n",
    "    return keep\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 헬퍼: 선택한 centers(원본 인덱스 기준)를 Dataset 내부 인덱스로 매핑\n",
    "# - SliceDataset2p5DMasked는 __init__에서 self.centers(원본 인덱스) 배열을 만든다.\n",
    "# - Subset은 \"dataset 내부 인덱스\"를 받으므로, 우리가 고른 centers(원본 인덱스)를\n",
    "#   dataset.centers에서의 위치로 변환해야 한다.\n",
    "# -------------------------------\n",
    "def map_centers_to_subset_idx(dataset: SliceDataset2p5DMasked, selected_centers: np.ndarray):\n",
    "    \"\"\"\n",
    "    dataset.centers (원본 인덱스 배열) 안에서 selected_centers(원본 인덱스)의 위치를 찾아\n",
    "    Subset 용 인덱스 리스트로 변환.\n",
    "    \"\"\"\n",
    "    # 빠른 매핑을 위해 원본 index -> dataset 내부 위치 dict 생성\n",
    "    pos_in_ds = {int(c): i for i, c in enumerate(dataset.centers.tolist())}\n",
    "    subset_idx = [pos_in_ds[int(c)] for c in selected_centers if int(c) in pos_in_ds]\n",
    "    return subset_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69cbd929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loaded] subjects=100, total_slices=317, pos=22, neg=295\n"
     ]
    }
   ],
   "source": [
    "X, y, subject_id = load_all_slices_from_tree(root_dir='./01_data/04_flair_preproc_slices/', \n",
    "                                             select_N=100,\n",
    "                                             choose_major_slice=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13050ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loaded] subjects=100, total_slices=317, pos=22, neg=295\n",
      "[Centers] train(full)=199 | train(bal)=26 | val=21 | test=97\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 하이퍼 파라미터\n",
    "seed = 42\n",
    "test_ratio = 0.3\n",
    "val_ratio = 0.1\n",
    "k = 1                       # 2.5D 문맥 크기 (i±k → 총 2k+1 채널)\n",
    "pad_mode = \"edge\"           # 경계 처리: \"edge\" 또는 \"zero\"\n",
    "drop_mixed = False          # True면 i±k 윈도우의 라벨이 모두 중심과 같을 때만 사용\n",
    "neg_ratio = 1.0             # 언더샘플링 비율(양성 1 당 음성 R)\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X, y, subject_id = load_all_slices_from_tree(root_dir='./01_data/04_flair_preproc_slices/', \n",
    "                                             select_N=100,\n",
    "                                             choose_major_slice=True)\n",
    "\n",
    "# 1) subject run/그룹 id 생성\n",
    "# runs, group_id = _compute_runs(subject_id)\n",
    "\n",
    "# 2) subject-level split (그룹 단위로 분할 → 누수 방지)\n",
    "train_idx, val_idx, test_idx = split_subjectwise_indices_ordered(\n",
    "    subject_id, test_ratio=test_ratio, val_ratio=val_ratio, seed=seed\n",
    ")\n",
    "\n",
    "# 3) 각 세트에서 유효 center 인덱스 생성 (subject 경계/라벨혼합 정책 반영)\n",
    "tr_centers_all = make_valid_centers(X[train_idx], y[train_idx], subject_id[train_idx],\n",
    "                                    k=k, pad_mode=pad_mode, drop_mixed=drop_mixed)\n",
    "va_centers_all = make_valid_centers(X[val_idx],   y[val_idx],   subject_id[val_idx],\n",
    "                                    k=k, pad_mode=pad_mode, drop_mixed=drop_mixed)\n",
    "te_centers_all = make_valid_centers(X[test_idx],  y[test_idx],  subject_id[test_idx],\n",
    "                                    k=k, pad_mode=pad_mode, drop_mixed=drop_mixed)\n",
    "\n",
    "# 4) (중요) 언더샘플링은 train center에만 적용 (재현성을 위해 seed 고정)\n",
    "tr_centers_bal = undersample_on_centers(tr_centers_all, y[train_idx], neg_ratio=neg_ratio, seed=seed)\n",
    "\n",
    "# 5) Dataset 구성\n",
    "#    주의: 여기서 Dataset은 \"split된 배열\"을 받음 (원본에서 인덱싱)\n",
    "ds_train_full = SliceDataset2p5DMasked(\n",
    "    X=X[train_idx], y=y[train_idx], sep_mask=subject_id[train_idx],\n",
    "    k=k, drop_mixed=drop_mixed, pad_mode=pad_mode\n",
    ")\n",
    "ds_val = SliceDataset2p5DMasked(\n",
    "    X=X[val_idx], y=y[val_idx], sep_mask=subject_id[val_idx],\n",
    "    k=k, drop_mixed=drop_mixed, pad_mode=pad_mode\n",
    ")\n",
    "ds_test = SliceDataset2p5DMasked(\n",
    "    X=X[test_idx], y=y[test_idx], sep_mask=subject_id[test_idx],\n",
    "    k=k, drop_mixed=drop_mixed, pad_mode=pad_mode\n",
    ")\n",
    "\n",
    "# 6) 우리가 고른 train centers만 사용하도록 Subset 구성\n",
    "#    - ds_train_full.centers: (유효 center의 \"원본 인덱스\") 배열\n",
    "#    - tr_centers_bal: 우리가 undersampling으로 고른 \"원본 인덱스\"\n",
    "subset_idx = map_centers_to_subset_idx(ds_train_full, tr_centers_bal)\n",
    "ds_train = Subset(ds_train_full, subset_idx)\n",
    "\n",
    "# (선택) 디버그 출력\n",
    "print(f\"[Centers] train(full)={len(ds_train_full)} | train(bal)={len(ds_train)} \"\n",
    "        f\"| val={len(ds_val)} | test={len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8186d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sample 0] center_global=15, label=0.0\n",
      "[sample 1] center_global=84, label=0.0\n",
      "[sample 2] center_global=165, label=0.0\n",
      "[sample 3] center_global=74, label=1.0\n",
      "[sample 4] center_global=122, label=1.0\n",
      "[sample 5] center_global=72, label=1.0\n"
     ]
    }
   ],
   "source": [
    "# 예: train 셋 몇 개 샘플을 꺼내 윈도우의 실제 인덱스와 라벨 확인\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl_debug = DataLoader(ds_train, batch_size=1, shuffle=False)\n",
    "for n, batch in enumerate(dl_debug):\n",
    "    x, y = batch           # x: (B=1, C=2k+1, H, W), y: (B,)\n",
    "    # ds_train은 Subset이므로, 원본 center는 used_centers[n]로 추적 가능\n",
    "    center_global = tr_centers_bal[n]\n",
    "    print(f\"[sample {n}] center_global={center_global}, label={y.item()}\")\n",
    "    if n == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7030e3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "796ac286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 46,  47,  48,  72,  73,  74,  75,  76, 118, 119, 120, 121, 122]),)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y[train_idx]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5478292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 84, 165, 74, 122, 72, 120, 182, 119, 108, 76, 126, 73, 46, 48, 47, 75, 169, 35, 109, 190, 118, 132, 177, 24, 121]\n"
     ]
    }
   ],
   "source": [
    "print(subset_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c0b6686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y[train_idx][subset_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d28f3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[13][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24c8bfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_centers_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28e5e880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 15,  84, 165,  74, 122,  72, 120, 182, 119, 108,  76, 126,  73,\n",
       "        46,  48,  47,  75, 169,  35, 109, 190, 118, 132, 177,  24, 121])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_centers_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c467037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train subject_id head: [ 1  1  1  2  2  2  3  3  3  6  6  6  7  7  7  8  8  8 13 13 13 14 14 14\n",
      " 16 16 16 17 17 17 19 19 19 20 20 20 21 21 21 23 23 23 24 24 24 25 25 25\n",
      " 25 25]\n",
      "val   subject_id head: [ 5  5  5 11 11 11 28 28 28 47 47 47 66 66 66 85 85 85 93 93 93]\n",
      "test  subject_id head: [ 0  0  0  4  4  4  9  9  9 10 10 10 12 12 12 15 15 15 18 18 18 22 22 22\n",
      " 26 26 26 30 30 30 31 31 31 33 33 33 39 39 39 39 39 39 39 40 40 40 42 42\n",
      " 42 44]\n",
      "[Train centers] total=199 (pos=13, neg=186)\n",
      "[Train centers (balanced)] total=26 (pos=13, neg=13)\n",
      "[ 15  84 165  74 122  72 120 182 119 108  76 126  73  46  48  47  75 169\n",
      "  35 109 190 118 132 177  24 121]\n",
      "[15, 84, 165, 74, 122, 72, 120, 182, 119, 108, 76, 126, 73, 46, 48, 47, 75, 169, 35, 109, 190, 118, 132, 177, 24, 121]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198]\n",
      "[15, 84, 165, 74, 122, 72, 120, 182, 119, 108, 76, 126, 73, 46, 48, 47, 75, 169, 35, 109, 190, 118, 132, 177, 24, 121]\n"
     ]
    }
   ],
   "source": [
    "# 0) 일관성 체크\n",
    "assert X.shape[0] == y.shape[0] == subject_id.shape[0]\n",
    "assert subject_id.ndim == 1 and np.issubdtype(subject_id.dtype, np.integer)\n",
    "\n",
    "# 1) split 후 subject 경계(run) 확인 (처음 50개만 보기)\n",
    "print(\"train subject_id head:\", subject_id[train_idx][:50])\n",
    "print(\"val   subject_id head:\", subject_id[val_idx][:50])\n",
    "print(\"test  subject_id head:\", subject_id[test_idx][:50])\n",
    "\n",
    "# 2) center 인덱스 체계 확인 (train split local)\n",
    "# ds_train_full.centers 가 split-local 인덱스라면 다음이 참이어야 함\n",
    "assert np.all((np.asarray(ds_train_full.centers) >= 0) &\n",
    "              (np.asarray(ds_train_full.centers) < len(ds_train_full)))\n",
    "\n",
    "# 3) undersampling 전/후 라벨 비율 로그\n",
    "y_tr_local = y[train_idx]\n",
    "pos_all = y_tr_local[tr_centers_all].sum()\n",
    "neg_all = len(tr_centers_all) - pos_all\n",
    "\n",
    "pos_bal = y_tr_local[tr_centers_bal].sum()\n",
    "neg_bal = len(tr_centers_bal) - pos_bal\n",
    "\n",
    "print(f\"[Train centers] total={len(tr_centers_all)} (pos={pos_all}, neg={neg_all})\")\n",
    "print(f\"[Train centers (balanced)] total={len(tr_centers_bal)} (pos={pos_bal}, neg={neg_bal})\")\n",
    "\n",
    "# print(subject_id)\n",
    "\n",
    "# print(subject_id[train_idx])\n",
    "# print(subject_id[val_idx])\n",
    "# print(subject_id[test_idx])\n",
    "\n",
    "print(tr_centers_bal)\n",
    "print(subset_idx)\n",
    "\n",
    "print(ds_train.dataset.y)\n",
    "print(ds_train.dataset.centers)\n",
    "print(ds_train.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36dafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_centers_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59bbe03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_idx.shape)\n",
    "print(y[train_idx])\n",
    "print(tr_centers.shape)\n",
    "\n",
    "print(train_idx)\n",
    "print(tr_centers)\n",
    "print(tr_centers_bal)\n",
    "print(y[train_idx][tr_centers_bal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59ec6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data_utils import undersample_negatives\n",
    "Xtr_bal, ytr_bal = undersample_negatives(Xtr, ytr, neg_ratio=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group_id)\n",
    "print(sep_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b36cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_idx.shape)\n",
    "print(val_idx.shape)\n",
    "print(test_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.argwhere(y==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[[75, 76, 77, 78, 79]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_mask[[75, 76, 77, 78, 79]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(sep_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435bd997",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = SliceDataset2p5DMasked(X, y, sep_mask, k=1, drop_mixed=False, pad_mode='edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = ds_train[75]\n",
    "print(xx[0][300, 300:310])\n",
    "print(xx[1][300, 300:310])\n",
    "print(xx[2][300, 300:310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "def _compute_runs(sep_mask: np.ndarray):\n",
    "    \"\"\"\n",
    "    sep_mask가 일정하게 유지되는 구간(= 같은 subject 구간)을 (start, end)로 반환.\n",
    "    sep_mask가 0/1로 번갈아 들어있어도, 값이 바뀌는 지점이 경계가 된다.\n",
    "    \n",
    "    - ex) sep_mask = array([0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1])\n",
    "    - ex) runs = [(0, 2), (3, 5), (6, 8), (9, 11)]\n",
    "    - ex) run_id = array([ 0,  0,  0,  1]\n",
    "    \"\"\"\n",
    "    \n",
    "    sep_mask = sep_mask.astype(np.int64)\n",
    "    if len(sep_mask) == 0:\n",
    "        return []\n",
    "    # 경계 지점 찾기: 값이 바뀌는 인덱스\n",
    "    boundaries = np.where(np.diff(sep_mask) != 0)[0]\n",
    "    starts = np.r_[0, boundaries + 1]\n",
    "    ends   = np.r_[boundaries, len(sep_mask) - 1]\n",
    "    runs = list(zip(starts, ends))  # 각 (start, end) 포함 구간\n",
    "    # 각 인덱스가 어느 run에 속하는지 맵핑 배열도 같이 반환하면 효율적\n",
    "    run_id = np.empty(len(sep_mask), dtype=np.int64)\n",
    "    for rid, (s, e) in enumerate(runs):\n",
    "        run_id[s:e+1] = rid\n",
    "    return runs, run_id\n",
    "\n",
    "runs, run_id = _compute_runs(sep_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61603ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615e4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [데이터셋 구성]\n",
    "# i-1, i, i+1 --> 트리오가 하나의 데이터\n",
    "# i-1, i+1 중, 하나라도 존재하지 않는 경우는 포함 x \n",
    "#  --> ex) [1, 2, 3, 4] 인 경우, (1, 2, 3), (2, 3, 4)는 가능. \n",
    "#  -->     (1, 2), (3, 4)는 각각 앞, 뒤로 데이터가 없으므로 제외.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fhv_deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
