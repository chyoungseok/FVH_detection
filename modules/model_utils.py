import os
from typing import Optional
import numpy as np
import torch
import torch.nn as nn
from torch.optim import lr_scheduler
from torchvision import models
from torchsummary import summary
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, precision_recall_curve
)
from sklearn.metrics import classification_report

from modules.plot_utils import save_eval_plots

# =========================================================
# ëª¨ë¸: ResNet18 (1ì±„ë„ ìž…ë ¥)
# =========================================================
def build_resnet_1ch(model_depth: int = 18, num_classes: int = 1) -> nn.Module:
    """
    torchvision resnet18ì„ í‘ë°± ì˜ë£Œì˜ìƒ(slice-level ì´ì§„ ë¶„ë¥˜)ì— ë§žê²Œ ìˆ˜ì •í•œ ëª¨ë¸ ìƒì„± í•¨ìˆ˜.
    - conv1: ê¸°ë³¸ ResNetì€ RGB(3ì±„ë„) ìž…ë ¥ìš© â†’ ì˜ë£Œì˜ìƒì€ 1ì±„ë„ì´ë¯€ë¡œ in_channels=1ë¡œ êµì²´
    - fc: ê¸°ë³¸ ResNetì€ ImageNet 1000í´ëž˜ìŠ¤ìš© â†’ ì´ì§„ ë¶„ë¥˜ì´ë¯€ë¡œ ì¶œë ¥=1(logit)ë¡œ êµì²´
    
    ** ê¸°ì¡´ resnet êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê³  ì‹¶ì€ ê²½ìš°
    from torchsummary import summary
    from torchvision import models

    model_resnet = models.resnet18(weights=None).to(device)
    summary(model_resnet, input_size=(3, 672, 672), device=device)
    print(model_resnet)
    
    """
    # ê¸°ë³¸ ResNet18 ë¶ˆëŸ¬ì˜¤ê¸° (ì‚¬ì „í•™ìŠµ weight ì‚¬ìš©í•˜ì§€ ì•ŠìŒ: ì˜ë£Œì˜ìƒ íŠ¹ì„± ìƒ ImageNet pretrainê³¼ ë„ë©”ì¸ ì°¨ì´ê°€ í¼)
    if model_depth == 18:
        model = models.resnet18(weights=None)
    elif model_depth == 34:
        model = models.resnet34(weights=None)
    elif model_depth == 50:
        model = models.resnet50(weights=None)
    elif model_depth == 101:
        model = models.resnet101(weights=None)
    elif model_depth == 152:
        model = models.resnet152(weights=None)

    # ì²« ë²ˆì§¸ convolution ë ˆì´ì–´ êµì²´
    # - ì›ëž˜ ì •ì˜: nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
    # - ì—¬ê¸°ì„œ 3ì€ RGB ìž…ë ¥ ì±„ë„ ìˆ˜ â†’ í‘ë°± ì˜ìƒì€ ì±„ë„ì´ 1ì´ë¯€ë¡œ in_channels=1ë¡œ ë³€ê²½
    # - out_channels=64ëŠ” ResNet í‘œì¤€ êµ¬ì¡°ë¥¼ ë§žì¶”ê¸° ìœ„í•œ ê°’ (ë°”ê¾¸ë©´ ë’¤ ë ˆì´ì–´ì™€ mismatch ë°œìƒ)
    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

    # ë§ˆì§€ë§‰ fully-connected ë ˆì´ì–´ êµì²´
    # - ì›ëž˜ ì •ì˜: nn.Linear(512, 1000) â†’ ImageNet 1000-class
    # - ì—¬ê¸°ì„œëŠ” ì´ì§„ ë¶„ë¥˜(FHV(+), FHV(-)) â†’ logit í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ë„ë¡ Linear(..., 1)
    # - BCEWithLogitsLossì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´, ì´ 1ê°œì˜ logitì— sigmoidë¥¼ ì”Œì›Œ í™•ë¥ [0,1]ë¡œ ë³€í™˜
    model.fc = nn.Linear(model.fc.in_features, num_classes)

    return model

def model_loss_optimizer_resnet(resnet_depth, device, weight_pos, lr, show_model_summary):
    # 1ì±„ë„ ìž…ë ¥(í‘ë°± FLAIR slice)ì— ë§žì¶˜ ResNet18 ëª¨ë¸ ìƒì„±
    # - conv1: in_channels=1 (ê¸°ë³¸ì€ 3, RGBìš©)
    # - fc: out_features=1 (ì´ì§„ ë¶„ë¥˜ â†’ ë‹¨ì¼ logit ì¶œë ¥)
    # - to(device): ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ GPU ë˜ëŠ” CPUë¡œ ì´ë™ (ìž…ë ¥ í…ì„œì™€ ê°™ì€ ë””ë°”ì´ìŠ¤ì—¬ì•¼ í•¨)
    model = build_resnet_1ch(model_depth=resnet_depth, num_classes=1).to(device)

    # í´ëž˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •ì„ ìœ„í•œ ê°€ì¤‘ì¹˜
    # - BCEWithLogitsLossì—ì„œ ì–‘ì„± í´ëž˜ìŠ¤ ì†ì‹¤ì— ê³±í•´ì§
    # - ì›ë³¸ ë°ì´í„°(ë¶ˆê· í˜•)ë¥¼ ê·¸ëŒ€ë¡œ ì“°ë©´ pos_weight â‰ˆ N_neg / N_pos ë¡œ ì„¤ì •
    # - BUT: ì´ë¯¸ 1:1 ì–¸ë”ìƒ˜í”Œë§ì„ ì ìš©í–ˆìœ¼ë¯€ë¡œ pos_weight=1.0 ìœ¼ë¡œ ë‘ë©´ ì¶©ë¶„
    pos_weight = torch.tensor([weight_pos], device=device)

    # ì†ì‹¤ í•¨ìˆ˜: BCE with Logits
    # - ëª¨ë¸ ì¶œë ¥(logit)ì„ ê·¸ëŒ€ë¡œ ìž…ë ¥ (sigmoid ë”°ë¡œ ì“°ì§€ ë§ ê²ƒ)
    # - ë‚´ë¶€ì—ì„œ sigmoid + binary cross entropyë¥¼ ì•ˆì •ì ìœ¼ë¡œ ê³„ì‚°
    # - pos_weight ì˜µì…˜ì„ í†µí•´ ì–‘ì„± í´ëž˜ìŠ¤ ì†ì‹¤ ê¸°ì—¬ë„ë¥¼ ì¡°ì ˆ
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

    # ì˜µí‹°ë§ˆì´ì €: Adam
    # - model.parameters(): ëª¨ë¸ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ ëŒ€ìƒìœ¼ë¡œ ì§€ì •
    # - lr: í•™ìŠµë¥ , ë³´í†µ 1e-3ì—ì„œ ì‹œìž‘
    # - weight_decay: L2 ì •ê·œí™” í•­ (ê°€ì¤‘ì¹˜ê°€ ê³¼ë„í•˜ê²Œ ì»¤ì§€ëŠ” ê²ƒì„ ë§‰ì•„ ê³¼ì í•© ë°©ì§€)
    #   â†’ ë„ˆë¬´ í¬ë©´ underfitting, ë„ˆë¬´ ìž‘ìœ¼ë©´ íš¨ê³¼ ì—†ìŒ. ì¼ë°˜ì ìœ¼ë¡œ 1e-4 ê¶Œìž¥
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    
    if show_model_summary:
        summary(model, input_size=(1, 672, 672))

    return model, criterion, optimizer

def init_lr_scheduler(
    scheduler_name: str,
    optimizer,
    lr_step_size: int,
    lr_gamma: float,
    lr_milestones: str,
    epochs: int,
    lr_T_max: int,
    lr_eta_min: float,
    plateau_mode: str,
    plateau_factor: float,
    plateau_patience: int,
):
    """
    í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ìœ í‹¸.
    - ì™œ í•„ìš”í•œê°€?
      Â· ê³ ì • lrë¡œ í•™ìŠµí•˜ë©´ íŠ¹ì • ì‹œì  ì´í›„ ìˆ˜ë ´ì´ ë”ëŽŒì§ˆ ìˆ˜ ìžˆìŒ â†’ lrì„ ì ì§„ì ìœ¼ë¡œ ì¤„ì—¬ ì¼ë°˜í™”/ìˆ˜ë ´ì„ ë•ëŠ”ë‹¤.
    - ì–´ë–¤ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì–¸ì œ ì“°ë‚˜?
      Â· "step"      : ì¼ì • epoch ê°„ê²©ìœ¼ë¡œ lr *= gamma (ë‹¨ìˆœÂ·ì§ê´€ì )
      Â· "multistep" : ì§€ì •í•œ epoch ë¦¬ìŠ¤íŠ¸ì—ì„œ lr *= gamma (ì „í˜•ì ì¸ ë¶„ê¸°ì  í•™ìŠµ)
      Â· "cosine"    : ì½”ì‚¬ì¸ ê³¡ì„ ìœ¼ë¡œ lr ê°ì†Œ(ë§ˆì§€ë§‰ì— eta_minìœ¼ë¡œ ìˆ˜ë ´; warm-restart ì—†ì´ ë‹¨ì¼ ì£¼ê¸°)
      Â· "plateau"   : ëª¨ë‹ˆí„°í•˜ëŠ” ì§€í‘œ(val_loss ë“±)ê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ lr *= factor (ìžë™ ê°ì‡ )
      Â· "none"      : ìŠ¤ì¼€ì¤„ëŸ¬ ë¯¸ì‚¬ìš© (None ë°˜í™˜)

    âš ï¸ ì£¼ì˜: í˜¸ì¶œ íƒ€ì´ë°ì´ ìŠ¤ì¼€ì¤„ëŸ¬ë§ˆë‹¤ ë‹¤ë¦„
      - StepLR/MultiStepLR/CosineAnnealingLR  â†’ epoch ë§ë¯¸ì—  scheduler.step()
      - ReduceLROnPlateau                    â†’ epoch ë§ë¯¸ì—  scheduler.step(val_loss)  # ëª¨ë‹ˆí„° ê°’ í•„ìˆ˜

    Args:
        scheduler_name   : {"none","step","multistep","cosine","plateau"}
                           (ê¸°ì¡´ ì½”ë“œì˜ lr_schedulerì™€ ì´ë¦„ì´ ê²¹ì³ í˜¼ë™ë˜ë¯€ë¡œ ëª…í™•ížˆ ë³€ê²½)
        optimizer        : torch.optim.Optimizer ì¸ìŠ¤í„´ìŠ¤
        lr_step_size     : StepLRì—ì„œ ëª‡ epochë§ˆë‹¤ ê°ì‡ í• ì§€ (ì˜ˆ: 5 â†’ ë§¤ 5 epochë§ˆë‹¤)
        lr_gamma         : ê°ì‡  ë¹„ìœ¨ (ì˜ˆ: 0.1 â†’ 10ë¶„ì˜ 1ë¡œ)
        lr_milestones    : MultiStepLRìš©, "10,20,30" í˜•íƒœì˜ ì½¤ë§ˆ êµ¬ë¶„ ë¬¸ìžì—´
        epochs           : ì´ í•™ìŠµ epoch ìˆ˜ (ê¸°ë³¸ê°’ ìƒì„±/ê²€ì¦ ì‹œ ì°¸ì¡°)
        lr_T_max         : CosineAnnealingLR ì£¼ê¸° ê¸¸ì´(ì—í­). 0/ìŒìˆ˜ë©´ epochs ì „ì²´ë¥¼ ì£¼ê¸°ë¡œ ì‚¬ìš©
        lr_eta_min       : CosineAnnealingLR ìµœì € í•™ìŠµë¥ 
        plateau_mode     : {"min","max"} Â· val_loss ëª¨ë‹ˆí„° ì‹œ "min", val_auc ëª¨ë‹ˆí„° ì‹œ "max"
        plateau_factor   : ReduceLROnPlateauì—ì„œ lrë¥¼ ê³±í•  ê°ì‡  ê³„ìˆ˜(ì˜ˆ: 0.5 â†’ ì ˆë°˜)
        plateau_patience : ê°œì„  ì—†ë‹¤ê³  íŒë‹¨í•˜ê¸° ì „ê¹Œì§€ ê¸°ë‹¤ë¦´ epoch ìˆ˜

    Returns:
        torch.optim.lr_scheduler ê°ì²´ ë˜ëŠ” None ("none"ì¸ ê²½ìš°)
    """
    # ì´ë¦„ ì¶©ëŒ/ê°€ë…ì„± ì°¨ì›ì—ì„œ torch.optim.lr_schedulerëŠ” _lrs ë³„ì¹­ìœ¼ë¡œ ì‚¬ìš©
    scheduler = None

    if scheduler_name == "none" or scheduler_name is None:
        # ìŠ¤ì¼€ì¤„ëŸ¬ ë¯¸ì‚¬ìš©: í˜¸ì¶œë¶€ì—ì„œ schedulerê°€ Noneì¸ì§€ ì²´í¬í•˜ê³  step()ì„ ë¶€ë¥´ì§€ ì•Šë„ë¡ ì²˜ë¦¬
        return None

    if scheduler_name == "step":
        # ë§¤ lr_step_size epochë§ˆë‹¤ lr = lr * lr_gamma
        scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)

    elif scheduler_name == "multistep":
        # "10,15" ê°™ì€ ë¬¸ìžì—´ì„ íŒŒì‹± â†’ [10, 15]
        milestones = [int(m) for m in lr_milestones.split(",") if m.strip().isdigit()]
        # ì‚¬ìš©ìžê°€ ì•ˆ ë„£ì—ˆì„ ë•Œì˜ ì•ˆì „í•œ ë””í´íŠ¸(ëŒ€ëžµ 60%, 85% ì§€ì ì—ì„œ ê°ì‡ )
        if len(milestones) == 0:
            milestones = [max(1, int(epochs * 0.6)), max(2, int(epochs * 0.85))]
        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=lr_gamma)

    elif scheduler_name == "cosine":
        # T_max: ì½”ì‚¬ì¸ í•œ ì£¼ê¸°ì˜ ê¸¸ì´(ì—í­). 0/ìŒìˆ˜ë©´ ì „ì²´ epochsë¥¼ í•œ ì£¼ê¸°ë¡œ ì‚¬ìš©.
        T_max = lr_T_max if lr_T_max and lr_T_max > 0 else epochs
        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=lr_eta_min)

        # ðŸ’¡ Tip: warmupì´ í•„ìš”í•˜ë©´ ë³„ë„ Warmup ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì•žë‹¨ì— êµ¬ì„±í•˜ê±°ë‚˜
        # OneCycleLR ê°™ì€ ëŒ€ì•ˆì„ ê³ ë ¤.

    elif scheduler_name == "plateau":
        # ëª¨ë‹ˆí„°í•˜ëŠ” ì§€í‘œê°€ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ lr *= plateau_factor ë¡œ ê°ì†Œ
        # mode="min": ë‚®ì„ìˆ˜ë¡ ì¢‹ë‹¤(ì˜ˆ: val_loss), mode="max": ë†’ì„ìˆ˜ë¡ ì¢‹ë‹¤(ì˜ˆ: val_auc)
        # patience: ê°œì„ ì´ ì—†ë‹¤ê³  íŒë‹¨í•˜ê¸° ì „ê¹Œì§€ ê¸°ë‹¤ë¦´ epoch ìˆ˜
        scheduler = lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode=plateau_mode,
            factor=plateau_factor,
            patience=plateau_patience,
        )

        # âš ï¸ í˜¸ì¶œë¶€ì—ì„œ scheduler.step(val_metric) í˜•íƒœë¡œ 'ëª¨ë‹ˆí„° ê°’'ì„ ë°˜ë“œì‹œ ì „ë‹¬í•´ì•¼ í•¨.
        # ì˜ˆ) val_lossë¥¼ ëª¨ë‹ˆí„°í•œë‹¤ë©´:  scheduler.step(val_loss)

    else:
        # ì˜¤íƒ€/ë¯¸ì§€ì› ì˜µì…˜ ë°©ì§€
        raise ValueError(f"Unknown scheduler_name: {scheduler_name}")

    return scheduler

def set_scaler(amp, device):
    """
    Initialize a gradient scaler for Automatic Mixed Precision (AMP) training.

    Args:
        amp (bool): If True, enable AMP support.
        device (str): Target device string ("cuda" or "cpu").

    Returns:
        torch.cuda.amp.GradScaler or None:
            - If amp=True and device="cuda": returns a GradScaler object
            - Otherwise: returns None (AMP not available on CPU)

    Notes:
        - AMP mixes FP16 and FP32 operations to improve speed and reduce memory usage,
          but may cause gradient underflow/overflow.
        - GradScaler dynamically scales the loss to stabilize FP16 training.
        - Usage in training loop:
            with torch.amp.autocast('cuda'):
                logits = model(imgs)
                loss = criterion(logits, labs)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
    """
    
    scaler = torch.amp.GradScaler('cuda') if (amp and device == "cuda") else None
    
    return scaler

def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):
    """
    Run one training epoch on the given dataset.

    Args:
        model (torch.nn.Module): í•™ìŠµí•  ëª¨ë¸ (ì˜ˆ: ResNet18 ë³€í˜•)
        loader (DataLoader): í•™ìŠµ ë°ì´í„°ì…‹ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì œê³µí•˜ëŠ” DataLoader
        criterion (torch.nn.Module): ì†ì‹¤ í•¨ìˆ˜ (BCEWithLogitsLoss ë“±)
        optimizer (torch.optim.Optimizer): ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•  ì˜µí‹°ë§ˆì´ì € (Adam, SGD ë“±)
        device (str): ì‹¤í–‰í•  ë””ë°”ì´ìŠ¤ ("cuda" ë˜ëŠ” "cpu")
        scaler (torch.cuda.amp.GradScaler, optional): 
            AMP(í˜¼í•© ì •ë°€) í•™ìŠµ ì‹œ gradient scalingì„ ìœ„í•œ GradScaler.
            Noneì´ë©´ ì¼ë°˜ í•™ìŠµ ìˆ˜í–‰.

    Returns:
        float: epoch ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ í‰ê·  ì†ì‹¤ ê°’

    Notes:
        - í•™ìŠµ ë‹¨ê³„ëŠ” ë‹¤ìŒ ìˆœì„œë¡œ ì§„í–‰:
            1. gradient ì´ˆê¸°í™” (zero_grad)
            2. forward pass â†’ loss ê³„ì‚°
            3. backward pass (gradient ê³„ì‚°)
            4. optimizer step (íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸)
            5. ë°°ì¹˜ ì†ì‹¤ ëˆ„ì 
        - AMP ì‚¬ìš© ì‹œ, autocast + GradScalerë¥¼ í†µí•´ FP16 ì—°ì‚°ì˜ ì•ˆì •ì„±ì„ í™•ë³´.
        - ë°˜í™˜ë˜ëŠ” ê°’ì€ ì „ì²´ ë°ì´í„°ì…‹ í‰ê·  ì†ì‹¤ë¡œ, í•™ìŠµ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ì— í™œìš©ë¨.
    """
    model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì • (Dropout, BatchNorm ë“± í›ˆë ¨ ì „ìš© ë™ìž‘ í™œì„±í™”)
    total_loss = 0.0  # epoch ì „ì²´ ì†ì‹¤ì„ ëˆ„ì í•  ë³€ìˆ˜

    # DataLoaderì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ êº¼ëƒ„
    for imgs, labs in loader:
        # ë°°ì¹˜ ë°ì´í„°ë¥¼ GPU/CPUë¡œ ì´ë™
        imgs = imgs.to(device, non_blocking=True)  
        labs = labs.to(device, non_blocking=True).view(-1, 1)  # (B,1) í˜•íƒœë¡œ ë§žì¶¤

        # ì´ì „ stepì—ì„œ ë‚¨ì•„ìžˆë˜ gradient ì´ˆê¸°í™”
        optimizer.zero_grad(set_to_none=True)

        # í˜¼í•© ì •ë°€(AMP) í•™ìŠµì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        if scaler is not None:
            with torch.amp.autocast('cuda'):
                logits = model(imgs)         # forward pass (logit ì¶œë ¥)
                loss = criterion(logits, labs)  # ì†ì‹¤ ê³„ì‚°
            scaler.scale(loss).backward()   # scaled gradient ê³„ì‚°
            scaler.step(optimizer)          # optimizer step (ìŠ¤ì¼€ì¼ë§ ë°˜ì˜)
            scaler.update()                 # ìŠ¤ì¼€ì¼ë§ ì¸ìž ê°±ì‹ 
        else:
            # AMPë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ í•™ìŠµ ë£¨í”„
            logits = model(imgs)
            loss = criterion(logits, labs)
            loss.backward()
            optimizer.step()

        # ë°°ì¹˜ ì†ì‹¤ ëˆ„ì 
        total_loss += loss.item() * imgs.size(0)

    # ì „ì²´ ë°ì´í„°ì…‹ í‰ê·  ì†ì‹¤ ë°˜í™˜
    return total_loss / len(loader.dataset)

@torch.no_grad() # í‰ê°€ ëª¨ë“œì—ì„œëŠ” gradientë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠìŒ (ë©”ëª¨ë¦¬/ì†ë„ ìµœì í™”)
def evaluate(model, loader, device, criterion: Optional[nn.Module] = None):
    """
    ëª¨ë¸ì„ ê²€ì¦/í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œ í‰ê°€í•˜ëŠ” í•¨ìˆ˜.

    Args:
        model (torch.nn.Module): í•™ìŠµëœ ëª¨ë¸
        loader (DataLoader): ê²€ì¦ ë˜ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì œê³µí•˜ëŠ” DataLoader
        device (str): ì‹¤í–‰ ìž¥ì¹˜ ("cuda" ë˜ëŠ” "cpu")
        criterion (nn.Module, optional): ì†ì‹¤ í•¨ìˆ˜. ì œê³µ ì‹œ í‰ê·  ì†ì‹¤ë„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜

    Returns:
        dict: í‰ê°€ ì§€í‘œì™€ ê´€ë ¨ ë°ì´í„°ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬
            - "acc"  : Accuracy (ì •í™•ë„)
            - "prec" : Precision (ì •ë°€ë„)
            - "rec"  : Recall (ìž¬í˜„ìœ¨)
            - "f1"   : F1-score
            - "auc"  : AUC (Area Under ROC Curve)
            - "cm"   : Confusion Matrix (í˜¼ë™ í–‰ë ¬)
            - "y_prob": ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥  (numpy array)
            - "y_true": ì‹¤ì œ ë¼ë²¨ (numpy array)
            - "loss" : í‰ê·  ì†ì‹¤ (criterionì´ ì£¼ì–´ì¡Œì„ ê²½ìš°ë§Œ í¬í•¨)
    """

    model.eval()
    all_probs, all_labels = [], []  # ë°°ì¹˜ë³„ í™•ë¥ ê³¼ ë¼ë²¨ì„ ëª¨ì•„ë‘˜ ë¦¬ìŠ¤íŠ¸
    total_loss, total_n = 0.0, 0    # ì†ì‹¤ í•©ê³„ì™€ ìƒ˜í”Œ ìˆ˜ (í‰ê·  ì†ì‹¤ ê³„ì‚°ìš©)

    # DataLoaderë¥¼ í†µí•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¶”ë¡ 
    for imgs, labs in loader:
        # ìž…ë ¥ ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        imgs = imgs.to(device, non_blocking=True)
        labs_f = labs.to(device, non_blocking=True).view(-1, 1)  # (B,) â†’ (B,1) ë³€í™˜ (ì†ì‹¤ ê³„ì‚° í˜¸í™˜ìš©)

        # forward pass (gradient ì—†ìŒ)
        logits = model(imgs)

        # ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì œê³µë°›ì€ ê²½ìš° â†’ ì†ì‹¤ë„ ê³„ì‚°í•˜ì—¬ ëˆ„ì 
        if criterion is not None:
            loss = criterion(logits, labs_f)
            total_loss += loss.item() * imgs.size(0)  # ë°°ì¹˜ ì†ì‹¤ í•©ê³„
            total_n += imgs.size(0)                   # ì „ì²´ ìƒ˜í”Œ ìˆ˜ ëˆ„ì 

        # sigmoid â†’ í™•ë¥ ë¡œ ë³€í™˜, shape (B,)
        probs = torch.sigmoid(logits).squeeze(1).cpu().numpy()
        all_probs.append(probs)           # ë°°ì¹˜ í™•ë¥  ì €ìž¥
        all_labels.append(labs.numpy())   # ë°°ì¹˜ ë¼ë²¨ ì €ìž¥ (CPU tensor â†’ numpy)

    # ë°°ì¹˜ë³„ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ arrayë¡œ ê²°í•©
    y_prob = np.concatenate(all_probs)           # ì˜ˆì¸¡ í™•ë¥ 
    y_true = np.concatenate(all_labels).astype(int)  # ì‹¤ì œ ë¼ë²¨
    y_pred = (y_prob >= 0.5).astype(int)         # 0.5 ìž„ê³„ê°’ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ ê²°ê³¼ ìƒì„±

    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    acc  = accuracy_score(y_true, y_pred)                     # ì •í™•ë„
    prec = precision_score(y_true, y_pred, zero_division=0)   # ì •ë°€ë„
    rec  = recall_score(y_true, y_pred, zero_division=0)      # ìž¬í˜„ìœ¨
    f1   = f1_score(y_true, y_pred, zero_division=0)          # F1-score

    # ROC AUC ê³„ì‚° (ë‹¨, ì–‘ì„±/ìŒì„±ì´ ëª¨ë‘ ì¡´ìž¬í•˜ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ â†’ NaN ì²˜ë¦¬)
    try:
        auc = roc_auc_score(y_true, y_prob)
    except ValueError:
        auc = float('nan')

    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(y_true, y_pred)

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
    out = {
        "acc": acc,
        "prec": prec,
        "rec": rec,
        "f1": f1,
        "auc": auc,
        "cm": cm,
        "y_prob": y_prob,
        "y_true": y_true
    }

    # ì†ì‹¤ í¬í•¨ ì—¬ë¶€ (criterionì„ ì œê³µí–ˆì„ ë•Œë§Œ)
    if criterion is not None and total_n > 0:
        out["loss"] = total_loss / total_n  # í‰ê·  ì†ì‹¤

    return out

def train_loop(
    out_dir,
    device,
    epochs,
    dl_train,
    dl_val,
    model,
    criterion,
    optimizer,
    scheduler,
    lr_scheduler,
    scaler,
    args,
    ):
    
    history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": [], "train_auc": [], "val_auc": [], "lr": []}
    best_f1, best_ckpt_path, best_metrics = -1.0, os.path.join(out_dir, "best_resnet18.pt"), None

    for epoch in range(1, epochs + 1):
        tr_loss = train_one_epoch(model, dl_train, criterion, optimizer, device, scaler=scaler)
        tr_metrics = evaluate(model, dl_train, device, criterion)
        val_metrics = evaluate(model, dl_val, device, criterion)
        history["train_loss"].append(tr_metrics.get("loss", tr_loss))
        history["val_loss"].append(val_metrics.get("loss", float('nan')))
        history["train_acc"].append(tr_metrics["acc"])
        history["val_acc"].append(val_metrics["acc"])
        history["train_auc"].append(tr_metrics.get("auc", float('nan')))
        history["val_auc"].append(val_metrics.get("auc", float('nan')))
        
        current_lr = optimizer.param_groups[0]['lr']
        history["lr"].append(current_lr)

        print(f"[Epoch {epoch:02d}] lr={current_lr:.3e} | tr_loss={tr_metrics.get('loss', tr_loss):.4f} | "
              f"val_loss={val_metrics.get('loss', float('nan')):.4f} | VAL: ACC={val_metrics['acc']:.3f} "
              f"PRE={val_metrics['prec']:.3f} REC={val_metrics['rec']:.3f} F1={val_metrics['f1']:.3f} "
              f"AUC={val_metrics['auc']:.3f}")

        if scheduler is not None:
            if lr_scheduler == "plateau":
                scheduler.step(val_metrics.get('loss', None))  # val loss ê¸°ì¤€
            else:
                scheduler.step()
        
        # Save best by validation F1
        if val_metrics['f1'] > best_f1:
            best_f1 = val_metrics['f1']
            best_metrics = val_metrics
            torch.save({
                "model": model.state_dict(),
                "epoch": epoch,
                "val_metrics": val_metrics,
                "args": vars(args),
                "seed": args.seed
            }, best_ckpt_path)
            
    
    return history, best_ckpt_path, best_metrics

def evaluation_on_test(best_ckpt_path, device, model, dl_test, criterion, out_dir):
    ckpt = torch.load(
        best_ckpt_path,
        map_location=device if isinstance(device, str) else None,
        weights_only=False 
        ) if os.path.exists(best_ckpt_path) else None
    
    if ckpt and 'model' in ckpt:
        model.load_state_dict(ckpt['model'])
    test_metrics = evaluate(model, dl_test, device, criterion)
    
    # Save predictions and metrics
    np.save(os.path.join(out_dir, "test_y_true.npy"), test_metrics["y_true"]) 
    np.save(os.path.join(out_dir, "test_y_prob.npy"), test_metrics["y_prob"]) 

    # Plots: ROC, PR, Confusion Matrix
    save_eval_plots(test_metrics["y_true"], test_metrics["y_prob"], out_dir, prefix="test", thr=0.5)
    
    return test_metrics


